{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550cc2e2",
   "metadata": {},
   "source": [
    "# ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5249044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain_chroma\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import fitz  # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "POST_DB_HOST = os.getenv(\"POST_DB_HOST\")\n",
    "POST_DB_NAME = os.getenv(\"POST_DB_NAME\")\n",
    "POST_DB_USER = os.getenv(\"POST_DB_USER\")\n",
    "POST_DB_PASSWD = os.getenv(\"POST_DB_PASSWD\")\n",
    "POST_DB_PORT = os.getenv(\"POST_DB_PORT\")\n",
    "# PostgreSQL ì—°ê²° ì—”ì§„ ìƒì„±\n",
    "post_engine = create_engine(f'postgresql://{POST_DB_USER}:{POST_DB_PASSWD}@{POST_DB_HOST}:{POST_DB_PORT}/{POST_DB_NAME}')\n",
    "db = psycopg2.connect(host=POST_DB_HOST, dbname=POST_DB_NAME,user=POST_DB_USER,password=POST_DB_PASSWD,port=POST_DB_PORT)\n",
    "cursor = db.cursor()\n",
    "db.autocommit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dcc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0c7bd",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54337bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì´ë ¥ì„œ (Resume)\n",
      "[ê¸°ë³¸ ì •ë³´]\n",
      "ì´ë¦„: í™ê¸¸ë™\n",
      "ì—°ë½ì²˜: 010-1234-5678\n",
      "ì´ë©”ì¼: honggildong.ai@gmail.com\n",
      "ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123\n",
      "[í•™ë ¥]\n",
      "ê³ ë ¤ëŒ€í•™êµ ì»´í“¨í„°í•™ê³¼ ì¡¸ì—… (2018.03 ~ 2024.02)\n",
      "GPA: 3.85 / 4.5\n",
      "ê´€ë ¨ ê³¼ëª©: ë¨¸ì‹ ëŸ¬ë‹, ë°ì´í„°ë§ˆì´ë‹, í†µê³„í•™, ë¹…ë°ì´í„°ì²˜ë¦¬, ë”¥ëŸ¬ë‹ ì´ë¡ ê³¼ ì‹¤ìŠµ\n",
      "[ê¸°ìˆ  ìŠ¤íƒ]\n",
      "Programming: Python, SQL, R\n",
      "Frameworks/Libraries: Scikit-learn, TensorFlow, PyTorch, Pandas, NumPy\n",
      "Tools: Jupyter, Git, Docker, Tableau\n",
      "DBMS: MySQL, MongoDB, Hadoop(HDFS), Spark\n",
      "Cloud: Google Colab, AWS EC2 & S3 (ê¸°ì´ˆ ìˆ˜ì¤€)\n",
      "[í”„ë¡œì íŠ¸ ê²½í—˜]\n",
      "1. ì‹ ë¬¸ ê¸°ì‚¬ ê¸°ë°˜ ê°ì„± ë¶„ì„ ëª¨ë¸ ê°œë°œ (2023.03 ~ 2023.06)\n",
      "ìì—°ì–´ì²˜ë¦¬(NLP) ê¸°ë°˜ ê°ì„± ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ\n",
      "KoNLPyì™€ Scikit-learnì„ ì´ìš©í•œ ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ\n",
      "ì •í™•ë„ 86% ë‹¬ì„±\n",
      "2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°œì¸ ë§ì¶¤í˜• ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ (2023.09 ~ 2023.12)\n",
      "ğŸ“„ ì´ë ¥ì„œ (Resume)\n",
      "1\n",
      "\n",
      "Content-based Filtering ë° Collaborative Filtering ê¸°ë²• ì ìš©\n",
      "Streamlitìœ¼ë¡œ ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„\n",
      "kaggle ë°ì´í„°ì…‹ ê¸°ë°˜, Precision@10: 0.73\n",
      "[ìê²©ì¦]\n",
      "ADsP (ë°ì´í„°ë¶„ì„ ì¤€ì „ë¬¸ê°€) â€“ 2023.08\n",
      "SQLD (SQL ê°œë°œì) â€“ 2024.01\n",
      "ğŸ“„ ì´ë ¥ì„œ (Resume)\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì´ë ¥ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "path = \"./data/ë¹…ë°ì´í„°AI_ì´ë ¥ì„œ.pdf\"\n",
    "doc = fitz.open(path)\n",
    "for page in doc:\n",
    "    resume_text = page.get_text()\n",
    "    print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_backup/rec_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df58314",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection_name = 'chroma_test'\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=collection_name,\n",
    "                                              metadata={\"hnsw:space\": \"cosine\"})\n",
    "except:\n",
    "    collection = chroma_client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794153a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=\"chroma_test\",  # âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    persist_directory=\"./chroma_data\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44ae351",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"chunk\"]\n",
    "    metadata = row.drop([\"chunk\"]).to_dict()\n",
    "    documents.append(Document(page_content=text, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3885ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250343/2890479165.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-IwGpHR0tmQinAuiSLBSiKYgU on tokens per min (TPM): Limit 1000000, Requested 1034038. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./chroma_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ì €ì¥ ê²½ë¡œ\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchroma_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    842\u001b[0m     ):\n\u001b[0;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/resources/embeddings.py:125\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    120\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apps/lib/python3.10/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-IwGpHR0tmQinAuiSLBSiKYgU on tokens per min (TPM): Limit 1000000, Requested 1034038. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_data\",  # ì €ì¥ ê²½ë¡œ\n",
    "    collection_name=\"chroma_test\"\n",
    ")\n",
    "\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d6af75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Processing batch 1/28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250343/1925841830.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Processing batch 2/28...\n",
      "â–¶ Processing batch 3/28...\n",
      "â–¶ Processing batch 4/28...\n",
      "â–¶ Processing batch 5/28...\n",
      "â–¶ Processing batch 6/28...\n",
      "â–¶ Processing batch 7/28...\n",
      "â–¶ Processing batch 8/28...\n",
      "â–¶ Processing batch 9/28...\n",
      "â–¶ Processing batch 10/28...\n",
      "â–¶ Processing batch 11/28...\n",
      "â–¶ Processing batch 12/28...\n",
      "â–¶ Processing batch 13/28...\n",
      "â–¶ Processing batch 14/28...\n",
      "â–¶ Processing batch 15/28...\n",
      "â–¶ Processing batch 16/28...\n",
      "â–¶ Processing batch 17/28...\n",
      "â–¶ Processing batch 18/28...\n",
      "â–¶ Processing batch 19/28...\n",
      "â–¶ Processing batch 20/28...\n",
      "â–¶ Processing batch 21/28...\n",
      "â–¶ Processing batch 22/28...\n",
      "â–¶ Processing batch 23/28...\n",
      "â–¶ Processing batch 24/28...\n",
      "â–¶ Processing batch 25/28...\n",
      "â–¶ Processing batch 26/28...\n",
      "â–¶ Processing batch 27/28...\n",
      "â–¶ Processing batch 28/28...\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ ì½”ë“œê°€ ë¶„ë‹¹ í† í° 100ë§Œì„ ë„˜ê²¼ê¸°ì— ì‚¬ìš©\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 500\n",
    "batch_documents = [documents[i:i+batch_size] for i in range(0, len(documents), batch_size)]\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectordb = None\n",
    "\n",
    "for i, batch in enumerate(batch_documents):\n",
    "    print(f\"â–¶ Processing batch {i+1}/{len(batch_documents)}...\")\n",
    "    if i == 0:\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=batch,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=\"./chroma_data\",\n",
    "            collection_name=\"chroma_test\"\n",
    "        )\n",
    "    else:\n",
    "        vectordb.add_documents(batch)\n",
    "\n",
    "    vectordb.persist()\n",
    "    sleep(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "beb1c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=\"chroma_test\",\n",
    "    persist_directory=\"./chroma_data\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1866447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "ë‹¹ì‹ ì€ ì¸ì¬ë§¤ì¹­ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì´ë ¥ì„œì— ê¸°ë°˜í•˜ì—¬ ì±„ìš©ê³µê³ ë¥¼ 5ê°œ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "- ë°˜ë“œì‹œ ì´ë ¥ì„œì— ê¸°ë°˜í•  ê²ƒ.\n",
    "- ì±„ìš©ê³µê³  ìƒì„¸ëŠ” ë°˜ë“œì‹œ ëª¨ë‘ ì¶œë ¥í•  ê²ƒ.\n",
    "- ì¶œë ¥ ì‹œ ì±„ìš©ê³µê³ ì˜ ì–‘ì‹ì„ ì‚¬ìš©í•  ê²ƒ.\n",
    "- ì±„ìš©ê³µê³  ì¶”ì²œ ì´ìœ ë¥¼ í•œì¤„ë¡œ ì„¤ëª…í•  ê²ƒ.                            \n",
    "- ì±„ìš©ê³µê³  ì „ì²´ ë‚´ìš©ì„ ê¸°ë°˜í•˜ì—¬ ë¶„ì„í•  ê²ƒ. ì œëª©ë§Œ ë³´ê³  íŒë‹¨í•˜ì§€ ë§ ê²ƒ.\n",
    "- page_contentì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•  ê²ƒ.\n",
    "\n",
    "### ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ\n",
    "- ì§ë¬´ëª… :\n",
    "- ê³µê³  ë²ˆí˜¸:                           \n",
    "- ì‚°ì—… ë¶„ì•¼ : \n",
    "- í•„ìš” ê¸°ìˆ  : \n",
    "- ì±„ìš©ê³µê³  ìƒì„¸ :\n",
    "- ì¶”ì²œ ì‚¬ìœ  :\n",
    "\n",
    "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì •ë³´ì— ê°€ì¥ ì í•©í•œ ì±„ìš© ê³µê³  5ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n",
    "ì¶œë ¥ í˜•ì‹ê³¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.                                          \n",
    "\n",
    "ì±„ìš©ê³µê³ :\n",
    "{context}\n",
    "\n",
    "ì´ë ¥ì„œ:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain1 = (\n",
    "    {\"context\": retriever | (lambda docs: \"\\n\\n\".join(\n",
    "            f\"{doc.metadata.get('rec_idx')}\\n{doc.metadata.get('page')} \\n{doc.page_content.strip()}\"\n",
    "            for doc in docs\n",
    "        )),\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84671cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Unnamed: 0': 4269, 'career': 'ê²½ë ¥ë¬´ê´€ Â· ì •ê·œì§', 'company_nm': '(ì£¼)ì†Œë§Œì‚¬', 'company_place': 'ê²½ê¸° ì„±ë‚¨ì‹œ ìˆ˜ì •êµ¬', 'education': 'ëŒ€í•™(2,3ë…„)â†‘', 'rec_idx': 50215560, 'recruit_kewdcdnm': \"['ì •ë³´ë³´ì•ˆ', 'ì†”ë£¨ì…˜', 'DLP', 'C++', 'Cì–¸ì–´']\"}, page_content='ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ SW ê°œë°œì(C++) ì±„ìš©\\n\\nì±„ìš©ê³µê³  ìƒì„¸')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b3906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ìì˜ ì´ë ¥ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì í•©í•œ ì±„ìš© ê³µê³  5ê°œë¥¼ ì¶”ì²œë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ë°ì´í„° ë¶„ì„ ë° ê°œë°œ ê´€ë ¨ ê²½í—˜ì´ ìˆìœ¼ë©°, ê´€ë ¨ ìê²©ì¦ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì í•©í•œ ì±„ìš© ê³µê³ ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. \n",
      "- ì§ë¬´ëª… : ë°ì´í„° ë¶„ì„ê°€\n",
      "- ê³µê³  ë²ˆí˜¸: 50226095\n",
      "- ì‚°ì—… ë¶„ì•¼ : ITê°œë°œÂ·ë°ì´í„°\n",
      "- í•„ìš” ê¸°ìˆ  : ë°ì´í„° ë¶„ì„, SQL, ë¨¸ì‹ ëŸ¬ë‹\n",
      "- ì±„ìš©ê³µê³  ìƒì„¸ : ãˆœLG CNSì—ì„œ ë°ì´í„° ë¶„ì„ê°€ë¥¼ ëª¨ì§‘í•˜ë©°, ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìš”êµ¬ë©ë‹ˆë‹¤.\n",
      "- ì¶”ì²œ ì‚¬ìœ  : ì‚¬ìš©ìì˜ ë°ì´í„° ë¶„ì„ ë° SQL ê´€ë ¨ ìê²©ì¦ê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì§ë¬´ ìš”êµ¬ì‚¬í•­ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. \n",
      "- ì§ë¬´ëª… : ë°ì´í„° ì—”ì§€ë‹ˆì–´\n",
      "- ê³µê³  ë²ˆí˜¸: 50226095\n",
      "- ì‚°ì—… ë¶„ì•¼ : ITê°œë°œÂ·ë°ì´í„°\n",
      "- í•„ìš” ê¸°ìˆ  : ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§, SQL, Python\n",
      "- ì±„ìš©ê³µê³  ìƒì„¸ : ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê²½í—˜ì´ ìˆëŠ” ì¸ì¬ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "- ì¶”ì²œ ì‚¬ìœ  : ì‚¬ìš©ìì˜ SQL ë° Python ê²½í—˜ì´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì§ë¬´ì— ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. \n",
      "- ì§ë¬´ëª… : ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´\n",
      "- ê³µê³  ë²ˆí˜¸: 50226095\n",
      "- ì‚°ì—… ë¶„ì•¼ : ITê°œë°œÂ·ë°ì´í„°\n",
      "- í•„ìš” ê¸°ìˆ  : ë¨¸ì‹ ëŸ¬ë‹, Python, TensorFlow\n",
      "- ì±„ìš©ê³µê³  ìƒì„¸ : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ ë° ìµœì í™” ê²½í—˜ì´ í•„ìš”í•˜ë©°, TensorFlow ì‚¬ìš© ê²½í—˜ì´ ìš”êµ¬ë©ë‹ˆë‹¤.\n",
      "- ì¶”ì²œ ì‚¬ìœ  : ì‚¬ìš©ìì˜ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ Python ê¸°ìˆ ì´ ì§ë¬´ì— ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. \n",
      "- ì§ë¬´ëª… : ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸\n",
      "- ê³µê³  ë²ˆí˜¸: 50226095\n",
      "- ì‚°ì—… ë¶„ì•¼ : ITê°œë°œÂ·ë°ì´í„°\n",
      "- í•„ìš” ê¸°ìˆ  : ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ë°ì´í„° ì‹œê°í™”\n",
      "- ì±„ìš©ê³µê³  ìƒì„¸ : ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì´ ìˆëŠ” ì¸ì¬ë¥¼ ëª¨ì§‘í•©ë‹ˆë‹¤.\n",
      "- ì¶”ì²œ ì‚¬ìœ  : ì‚¬ìš©ìì˜ ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ê²½í—˜ì´ ì§ë¬´ ìš”êµ¬ì‚¬í•­ê³¼ ë¶€í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. \n",
      "- ì§ë¬´ëª… : SQL ê°œë°œì\n",
      "- ê³µê³  ë²ˆí˜¸: 50226095\n",
      "- ì‚°ì—… ë¶„ì•¼ : ITê°œë°œÂ·ë°ì´í„°\n",
      "- í•„ìš” ê¸°ìˆ  : SQL, ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬\n",
      "- ì±„ìš©ê³µê³  ìƒì„¸ : SQL ê°œë°œ ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ê²½í—˜ì´ ìˆëŠ” ì¸ì¬ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "- ì¶”ì²œ ì‚¬ìœ  : ì‚¬ìš©ìì˜ SQLD ìê²©ì¦ê³¼ SQL ê²½í—˜ì´ ì§ë¬´ì— ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ìƒ 5ê°œì˜ ì±„ìš© ê³µê³ ëŠ” ì‚¬ìš©ìì˜ ì´ë ¥ì„œì™€ ì˜ ë§ëŠ” ì§ë¬´ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ê³µê³ ì˜ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì‹œê³  ì§€ì›ì„ ê³ ë ¤í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "query = resume_text\n",
    "answer1 = rag_chain1.invoke(query)\n",
    "print(answer1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
